#!/usr/bin/env python3
"""
excel_nl_query_claude_full.py

Query an Excel file using Claude (Anthropic) with:
- Natural language -> JSON query via Claude
- Fuzzy column-name mapping
- AND / OR filters
- Column type inference and coercion (datetime, numeric, boolean)
- Value parsing so comparisons work regardless of input formats (e.g., dates)

Usage:
  export ANTHROPIC_API_KEY="..."
  python excel_nl_query_claude_full.py data.xlsx -q "Show top 10 orders where city is Bangalore OR city contains 'Delhi' return customer_name and order_amount sorted by order_amount desc"
"""

import os
import json
import argparse
import difflib
from typing import Any, Dict, List, Optional, Tuple
import pandas as pd
import numpy as np
from dateutil import parser as dateparser
from anthropic import Anthropic

# ---------------- CONFIG ----------------
DEFAULT_MODEL = os.environ.get("ANTHROPIC_MODEL", "claude-3-5-sonnet-20240620")
API_KEY_ENV = "ANTHROPIC_API_KEY"
MAX_TOKENS = 1200
SAMPLE_ROWS = 6
FUZZY_CUTOFF = 0.6  # 0-1, higher = stricter
PREFER_DAYFIRST_FALLBACK = True
COERCE_NUMERIC_STRINGS = True
# ----------------------------------------

SYSTEM_PROMPT_TEMPLATE = """
You are a data-query assistant. You will receive a table schema (column names and dtypes),
a few sample rows, and a user's natural-language question.

Return ONLY valid JSON (no markdown, no backticks) using this schema:

{
  "select": ["column1","column2", ...] OR "ALL",
  "filter": {
    "and": [ { "column": "colname", "op":"contains|eq|gt|gte|lt|lte|in", "value": ... }, ... ],
    "or":  [ { "column": "colname", "op":"contains|eq|gt|gte|lt|lte|in", "value": ... }, ... ]
  },
  "sort": { "column": "asc|desc" },
  "limit": integer
}

Conventions:
- "contains" = case-insensitive substring.
- "in" = a JSON array or comma-separated list.
- Column names may be approximate; the executor will map them to actual columns.
- NEVER include code or commentary. Output must be only parsable JSON.

Table schema (column: dtype):
{schema}

Sample rows:
{sample}
""".strip()

# ---------------- Type inference & coercion helpers ----------------

def infer_column_type(series: pd.Series) -> str:
    """Return "datetime", "numeric", "boolean", or "string"."""
    if pd.api.types.is_datetime64_any_dtype(series):
        return "datetime"
    if pd.api.types.is_bool_dtype(series):
        return "boolean"
    if pd.api.types.is_numeric_dtype(series):
        return "numeric"

    sample_vals = series.dropna().astype(str).head(50).tolist()
    numeric_count = 0
    date_count = 0
    bool_count = 0
    for v in sample_vals:
        vs = v.strip()
        if vs == "":
            continue
        try:
            float(vs.replace(",", ""))
            numeric_count += 1
            continue
        except Exception:
            pass
        if vs.lower() in {"true", "false", "yes", "no", "y", "n", "1", "0"}:
            bool_count += 1
            continue
        try:
            _ = dateparser.parse(vs, fuzzy=False)
            date_count += 1
            continue
        except Exception:
            pass

    n = max(1, len(sample_vals))
    if date_count >= max(2, n // 4):
        return "datetime"
    if numeric_count >= max(2, n // 4):
        return "numeric"
    if bool_count >= max(1, n // 6):
        return "boolean"
    return "string"

def try_parse_date(value: Any, prefer_dayfirst: bool = PREFER_DAYFIRST_FALLBACK) -> Optional[pd.Timestamp]:
    if pd.isna(value):
        return pd.NaT
    if isinstance(value, pd.Timestamp):
        return value
    if isinstance(value, (int, float, np.integer, np.floating)):
        # Avoid interpreting small ints as timestamps; try seconds-based only if large
        try:
            return pd.to_datetime(value, unit='s', errors='coerce')
        except Exception:
            return pd.NaT
    s = str(value).strip()
    if s == "":
        return pd.NaT
    try:
        return pd.Timestamp(dateparser.parse(s, fuzzy=False))
    except Exception:
        if prefer_dayfirst:
            try:
                return pd.Timestamp(dateparser.parse(s, dayfirst=True, fuzzy=False))
            except Exception:
                return pd.NaT
        return pd.NaT

def parse_value_for_column(value: Any, target_type: str) -> Any:
    if isinstance(value, list):
        return [parse_value_for_column(v, target_type) for v in value]
    if value is None or (isinstance(value, float) and np.isnan(value)):
        return None
    if target_type == "datetime":
        parsed = try_parse_date(value)
        return parsed if not pd.isna(parsed) else value
    if target_type == "numeric":
        if isinstance(value, (int, float, np.integer, np.floating)):
            return float(value)
        vs = str(value).replace(",", "").strip()
        try:
            return float(vs)
        except Exception:
            return value
    if target_type == "boolean":
        vs = str(value).strip().lower()
        if vs in {"true", "yes", "y", "1"}:
            return True
        if vs in {"false", "no", "n", "0"}:
            return False
        return value
    return str(value)

def coerce_dataframe_types(df: pd.DataFrame, prefer_dayfirst: bool = PREFER_DAYFIRST_FALLBACK) -> Tuple[pd.DataFrame, Dict[str,str]]:
    df_copy = df.copy()
    inferred: Dict[str,str] = {}
    for col in df_copy.columns:
        s = df_copy[col]
        typ = infer_column_type(s)
        inferred[col] = typ
        if typ == "datetime":
            try:
                coerced = pd.to_datetime(s, errors="coerce", infer_datetime_format=True, dayfirst=False)
                frac = coerced.notna().mean() if len(coerced)>0 else 0
                if frac < 0.5 and prefer_dayfirst:
                    coerced_alt = pd.to_datetime(s, errors="coerce", infer_datetime_format=True, dayfirst=True)
                    if coerced_alt.notna().mean() > frac:
                        coerced = coerced_alt
                df_copy[col] = coerced
            except Exception:
                df_copy[col] = s.apply(lambda x: try_parse_date(x, prefer_dayfirst))
        elif typ == "numeric":
            df_copy[col] = pd.to_numeric(s.astype(str).str.replace(",", ""), errors="coerce")
        elif typ == "boolean":
            def to_bool(x):
                if pd.isna(x):
                    return np.nan
                xs = str(x).strip().lower()
                if xs in {"true","yes","y","1"}:
                    return True
                if xs in {"false","no","n","0"}:
                    return False
                return np.nan
            df_copy[col] = s.apply(to_bool)
        else:
            df_copy[col] = s.astype(object)
    return df_copy, inferred

# ---------------- Model / JSON helpers ----------------

def build_schema_and_sample(df: pd.DataFrame):
    schema = [{"column": c, "dtype": str(df[c].dtype)} for c in df.columns]
    sample = df.head(SAMPLE_ROWS).to_dict(orient="records")
    return schema, sample

def call_claude_get_json(schema, sample, user_question: str, model: str = DEFAULT_MODEL) -> Dict:
    if API_KEY_ENV not in os.environ:
        raise RuntimeError(f"Please set {API_KEY_ENV} in your environment.")
    client = Anthropic(api_key=os.environ.get(API_KEY_ENV))
    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(schema=json.dumps(schema, indent=2), sample=json.dumps(sample, indent=2))
    resp = client.messages.create(
        model=model,
        system=system_prompt,
        messages=[{"role":"user", "content": f"User question:\n{user_question}\n\nReturn ONLY JSON."}],
        max_tokens=MAX_TOKENS,
    )
    text = "".join(block.text for block in resp.content if getattr(block, "type", None) == "text")
    return extract_json_from_text(text)

def extract_json_from_text(text: str) -> Dict:
    text = text.strip()
    if text.startswith("```"):
        text = text.strip("`")
        if text.lower().startswith("json"):
            text = text[4:].lstrip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidate = text[start:end+1]
            return json.loads(candidate)
        raise ValueError("Unable to parse JSON from model response.")

# ---------------- Fuzzy column matching ----------------

def map_column_name(requested: str, columns: List[str], cutoff: float = FUZZY_CUTOFF) -> Optional[str]:
    if requested is None:
        return None
    for c in columns:
        if c.lower() == str(requested).lower():
            return c
    matches = difflib.get_close_matches(str(requested), columns, n=1, cutoff=cutoff)
    if matches:
        return matches[0]
    tokens = [t.lower() for t in str(requested).replace("_", " ").split()]
    for c in columns:
        lowc = c.lower()
        if any(tok for tok in tokens if tok and tok in lowc):
            return c
    return None

def normalize_query_columns(query: Dict, df_columns: List[str]) -> Dict:
    warnings = []
    q = dict(query)  # shallow copy
    fil = q.get("filter")
    if isinstance(fil, dict):
        for key in ("and", "or"):
            if key in fil and isinstance(fil[key], list):
                newlist = []
                for cond in fil[key]:
                    cond = dict(cond)
                    col = cond.get("column")
                    mapped = map_column_name(col, df_columns)
                    if mapped and mapped != col:
                        warnings.append(f"Mapped requested column '{col}' -> '{mapped}'")
                    if mapped is None:
                        warnings.append(f"Could not map requested column '{col}'; leaving as-is (may error).")
                        cond["column_mapped"] = col
                    else:
                        cond["column_mapped"] = mapped
                    newlist.append(cond)
                fil[key] = newlist
        q["filter"] = fil
    q["__warnings"] = warnings
    return q

# ---------------- Condition evaluation with types ----------------

def evaluate_condition_with_types(df: pd.DataFrame, cond: Dict, inferred_types: Dict[str,str]) -> pd.Series:
    col = cond.get("column_mapped") or cond.get("column")
    op = cond.get("op", "eq")
    val = cond.get("value")
    if col not in df.columns:
        return pd.Series(False, index=df.index)
    col_type = inferred_types.get(col, "string")
    series = df[col]
    parsed_val = parse_value_for_column(val, col_type)

    # contains
    if op == "contains":
        return series.astype(str).str.contains(str(parsed_val), case=False, na=False)

    if op == "eq":
        if col_type == "datetime":
            if isinstance(parsed_val, pd.Timestamp):
                return series == parsed_val
            return series.astype(str) == str(val)
        if col_type == "numeric":
            try:
                target = float(parsed_val)
                return pd.to_numeric(series, errors="coerce") == target
            except Exception:
                return series.astype(str) == str(val)
        if col_type == "boolean":
            return series == parsed_val
        return series.astype(str) == str(parsed_val)

    if op in {"gt", "gte", "lt", "lte"}:
        if col_type == "numeric":
            try:
                target = float(parsed_val)
                colnum = pd.to_numeric(series, errors="coerce")
                if op == "gt":
                    return colnum > target
                if op == "gte":
                    return colnum >= target
                if op == "lt":
                    return colnum < target
                return colnum <= target
            except Exception:
                return pd.Series(False, index=df.index)
        if col_type == "datetime":
            if isinstance(parsed_val, pd.Timestamp):
                if op == "gt":
                    return series > parsed_val
                if op == "gte":
                    return series >= parsed_val
                if op == "lt":
                    return series < parsed_val
                return series <= parsed_val
            return pd.Series(False, index=df.index)
        try:
            if op == "gt":
                return series.astype(str) > str(parsed_val)
            if op == "gte":
                return series.astype(str) >= str(parsed_val)
            if op == "lt":
                return series.astype(str) < str(parsed_val)
            return series.astype(str) <= str(parsed_val)
        except Exception:
            return pd.Series(False, index=df.index)

    if op == "in":
        if isinstance(parsed_val, list):
            items = parsed_val
        elif isinstance(parsed_val, str) and "," in parsed_val:
            items = [v.strip() for v in parsed_val.split(",") if v.strip()]
        else:
            items = [parsed_val]
        if col_type == "datetime":
            parsed_ts = []
            for it in items:
                if isinstance(it, pd.Timestamp):
                    parsed_ts.append(it)
                else:
                    parsed_ts.append(try_parse_date(it))
            parsed_ts = [p for p in parsed_ts if not pd.isna(p)]
            if parsed_ts:
                return series.isin(parsed_ts)
            return series.astype(str).isin([str(x) for x in items])
        if col_type == "numeric":
            series_num = pd.to_numeric(series, errors="coerce")
            parsed_nums = []
            for it in items:
                try:
                    parsed_nums.append(float(it))
                except Exception:
                    pass
            if parsed_nums:
                return series_num.isin(parsed_nums)
            return series.astype(str).isin([str(x) for x in items])
        return series.astype(str).isin([str(x) for x in items])

    return pd.Series(False, index=df.index)

# ---------------- Query execution ----------------

def execute_query(df_raw: pd.DataFrame, query: Dict) -> pd.DataFrame:
    # Pre-coerce types
    df, inferred_types = coerce_dataframe_types(df_raw)
    # Normalize column names in query (fuzzy)
    q_norm = normalize_query_columns(query, list(df.columns))
    warnings = q_norm.get("__warnings", [])
    fil = q_norm.get("filter")

    # Legacy flat filter detection: {"city__contains": "Delhi"}
    if (not fil or not isinstance(fil, dict)) and isinstance(query.get("filter"), dict):
        legacy = query.get("filter")
        if any("__" in k for k in legacy.keys()):
            and_list = []
            for k, v in legacy.items():
                if "__" in k:
                    col, op = k.split("__", 1)
                    mapped = map_column_name(col, list(df.columns))
                    and_list.append({"column_mapped": mapped or col, "op": op, "value": v})
            fil = {"and": and_list}
            q_norm["filter"] = fil

    mask = pd.Series(True, index=df.index)
    if isinstance(q_norm.get("filter"), dict):
        f = q_norm["filter"]
        if "and" in f and isinstance(f["and"], list) and f["and"]:
            for cond in f["and"]:
                cond_mask = evaluate_condition_with_types(df, cond, inferred_types)
                mask = mask & cond_mask
        if "or" in f and isinstance(f["or"], list) and f["or"]:
            or_mask = pd.Series(False, index=df.index)
            for cond in f["or"]:
                or_mask = or_mask | evaluate_condition_with_types(df, cond, inferred_types)
            mask = (mask & or_mask) if ("and" in f and f["and"]) else or_mask

    result = df[mask]

    # SELECT
    select = q_norm.get("select")
    if select and select != "ALL":
        mapped_select = []
        for s in select:
            mapped = map_column_name(s, list(df.columns))
            if mapped:
                mapped_select.append(mapped)
                if mapped != s:
                    warnings.append(f"Mapped select column '{s}' -> '{mapped}'")
            else:
                warnings.append(f"Select column '{s}' not found; ignoring.")
        if mapped_select:
            result = result.loc[:, mapped_select]

    # SORT
    sort_spec = q_norm.get("sort")
    if isinstance(sort_spec, dict) and sort_spec:
        col, order = list(sort_spec.items())[0]
        mapped = map_column_name(col, list(df.columns))
        if mapped:
            try:
                result = result.sort_values(by=mapped, ascending=(order.lower()=="asc"))
            except Exception as e:
                warnings.append(f"Sort failed on '{col}': {e}")
        else:
            warnings.append(f"Sort column '{col}' not found; skipping sort.")

    # LIMIT
    limit = q_norm.get("limit")
    if isinstance(limit, int) and limit > 0:
        result = result.head(limit)

    # Print warnings
    if warnings:
        print("=== Executor warnings ===")
        for w in warnings:
            print(" -", w)
    return result

# ---------------- CLI ----------------

def main():
    parser = argparse.ArgumentParser(description="Query an Excel file using Claude (natural language).")
    parser.add_argument("excel_path", help="Path to Excel (.xlsx)")
    parser.add_argument("-s", "--sheet", help="Sheet name", default=None)
    parser.add_argument("-q", "--question", help="Natural language question", required=True)
    args = parser.parse_args()

    if API_KEY_ENV not in os.environ:
        raise RuntimeError(f"Please set {API_KEY_ENV} in your environment.")

    df = pd.read_excel(args.excel_path, sheet_name=args.sheet)
    schema, sample = build_schema_and_sample(df)
    query_json = call_claude_get_json(schema, sample, args.question)
    print("=== JSON returned by Claude ===")
    print(json.dumps(query_json, indent=2, default=str))
    print("\n=== Executing query ===")
    result = execute_query(df, query_json)
    if result.empty:
        print("No rows matched.")
    else:
        # Show first 200 rows
        print(result.head(200).to_string(index=False))

if __name__ == "__main__":
    main()